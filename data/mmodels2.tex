\begin{table}[!htbp]
 \centering
 \caption{List of models selected by TPOT in order of precision. All models except those generated by TPOT light (LR and DT) were overfitted. \newline \textbf{Name} (\_vif=Treated for multi-correlation \ref{ssec:multicorrel}), (\_cap2=outliers capped \ref{ssec:outliers})}
 \label{datammodels2tab} 
  \begin{tabular}
{| 
 p{\dimexpr0.18\textwidth-2\tabcolsep-\arrayrulewidth\relax}| 
 p{\dimexpr0.15\textwidth-2\tabcolsep-\arrayrulewidth\relax}| 
 p{\dimexpr0.15\textwidth-2\tabcolsep-\arrayrulewidth\relax}| 
 p{\dimexpr0.52\textwidth-2\tabcolsep-\arrayrulewidth\relax}| 
}\hline 
\textbf{Name} &\textbf{Precision CONFIRMED} &\textbf{Accuracy} &\textbf{Python sklearn model} \\ \hline 
GB\_vif\_cap2&0.835 &0.859 &sklearn.ensemble.GradientBoostingClassifier \\ \hline 
\textbf{GB\_vif} &0.830 &0.859 &sklearn.ensemble.GradientBoostingClassifier \\ \hline 
\textbf{RF\_vif} &0.817 &0.855 &sklearn.ensemble.RandomForestClassifier \\ \hline 
GB &0.816 &0.855 &sklearn.ensemble.GradientBoostingClassifier \\ \hline 
RF\_vif\_cap2 &0.811 &0.859 &sklearn.ensemble.RandomForestClassifier \\ \hline 
GBtest &0.810 &0.863 &sklearn.ensemble.GradientBoostingClassifier \\ \hline 
RF &0.806 &0.855 &sklearn.ensemble.RandomForestClassifier \\ \hline 
\textbf{LR} &0.804 &0.85 &sklearn.linear\_model.LogisticRegression \\ \hline 
\textbf{DT} &0.793 &0.844 &sklearn.tree.DecisionTreeClassifier \\ \hline 
LR\_vif\_cap2 &0.791 &0.834 &sklearn.linear\_model.LogisticRegression \\ \hline 
DT\_vif &0.769 &0.809 &sklearn.tree.DecisionTreeClassifier \\ \hline 
BernoulliNB &0.441 &0.555 &sklearn.naive\_bayes.BernoulliNB \\ \hline 
KNeighbors Classifier &0.416 &0.522 &sklearn.neighbors.KNeighborsClassifier \\ \hline 
GaussianNB &0.348 &0.409 &sklearn.naive\_bayes.GaussianNB \\ \hline 
%DT\_vif\_cap2 &- &0.804 &sklearn.tree.DecisionTreeClassifier \\ \hline 
\end{tabular} 
\end{table}