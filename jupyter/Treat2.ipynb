{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/TK.csv to data/TK_npca.csv with col 'pca' var =[0.99994871] [5.05165153e+12]\n"
     ]
    }
   ],
   "source": [
    "#!/home/admin/anaconda3/bin/python3\n",
    "#   author:martinmhan@yahoo.com date:  17/06/2020\n",
    "#   Copyright (C) <2020>  <Martin Mohan>\n",
    "#\n",
    "#   This program is free software; you can redistribute it and/or modify\n",
    "#   it under the terms of the GNU General Public License as published by\n",
    "#   the Free Software Foundation; either version 3 of the License, or\n",
    "#   (at your option) any later version.\n",
    "#\n",
    "#   This program is distributed in the hope that it will be useful,\n",
    "#   but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "#   GNU General Public License for more details.\n",
    "#\n",
    "#   You should have received a copy of the GNU General Public License\n",
    "#   along with this program; if not, write to the Free Software Foundation,\n",
    "#   Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301  USA\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse,sys,re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn; seaborn.set()\n",
    "import mmutils\n",
    "import myfit\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "class Treat2():\n",
    "    \"\"\" \n",
    "    This class will modify the input file depending on the user selection.\n",
    "    After modification name of the output file is modifed to reflect the change.\n",
    "    e.g. if outlier removal applied ifile.csv -> ifile_out.csv\n",
    "    Only one method can be called at time.\n",
    "    The TCE file is modified the same way as the TK file\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,TK):\n",
    "        \"\"\"\n",
    "        The input file is saved\n",
    "\n",
    "        \"\"\"\n",
    "        self.TK=TK\n",
    "        self.TCE1=TK.replace(\"TK\",\"TCE1\")\n",
    "        \n",
    "    def bin(self):\n",
    "        ''' Merge FP and CANDIDATE - \"b\" added to start of all files. '''\n",
    "        bTK=self.TK.replace(\"data/\", \"data/b\")\n",
    "        df = pd.read_csv(self.TK,comment= '#')\n",
    "        df[\"koi_disposition\"] = df[\"koi_disposition\"].replace(to_replace=\"CANDIDATE\",value=\"FALSE POSITIVE\")\n",
    "        df.to_csv(bTK,index=False)\n",
    "        \n",
    "        # Just rename TCE1 it has no koi_disposition\n",
    "        bTCE1=self.TCE1.replace(\"data/\",\"data/b\")\n",
    "        df = pd.read_csv(self.TCE1,comment= '#') \n",
    "        df.to_csv(bTCE1,index=False)\n",
    "        return bTK,bTCE1\n",
    "    \n",
    "    def get_vif(self,exogs, data):\n",
    "        ''' \n",
    "        This is here for reference (removing vif is done iteratively one at a time.\n",
    "        From https://stackoverflow.com/questions/42658379/variance-inflation-factor-in-python\n",
    "        Return VIF (variance inflation factor) DataFrame\n",
    "\n",
    "        Args:\n",
    "        exogs (list): list of exogenous/independent variables\n",
    "        data (DataFrame): the df storing all variables\n",
    "\n",
    "        Returns:\n",
    "        VIF and Tolerance DataFrame for each exogenous variable\n",
    "\n",
    "        Notes:\n",
    "        Assume we have a list of exogenous variable [X1, X2, X3, X4].\n",
    "        To calculate the VIF and Tolerance for each variable, we regress\n",
    "        each of them against other exogenous variables. For instance, the\n",
    "        regression model for X3 is defined as:\n",
    "                        X3 ~ X1 + X2 + X4\n",
    "        And then we extract the R-squared from the model to calculate:\n",
    "                    VIF = 1 / (1 - R-squared)\n",
    "                    Tolerance = 1 - R-squared\n",
    "        The cutoff to detect multicollinearity:\n",
    "                    VIF > 10 or Tolerance < 0.1\n",
    "        '''\n",
    "\n",
    "        # initialize dictionaries\n",
    "        vif_dict, tolerance_dict = {}, {}\n",
    "        # create formula for each exogenous variable\n",
    "        for exog in exogs:\n",
    "            not_exog = [i for i in exogs if i != exog]\n",
    "            formula = f\"{exog} ~ {' + '.join(not_exog)}\"\n",
    "            # extract r-squared from the fit\n",
    "            r_squared = smf.ols(formula, data=data).fit().rsquared\n",
    "\n",
    "            # calculate VIF\n",
    "            vif = 1/(1 - r_squared)\n",
    "            vif_dict[exog] = vif\n",
    "\n",
    "            # calculate tolerance\n",
    "            tolerance = 1 - r_squared\n",
    "            tolerance_dict[exog] = tolerance\n",
    "        # return VIF DataFrame\n",
    "        df_vif = pd.DataFrame({'VIF': vif_dict, 'Tolerance': tolerance_dict})\n",
    "        return df_vif \n",
    "    \n",
    "    def cap(self,iv):\n",
    "        ''' Cap all outliers Interquartile range IQR*1.5 '''        \n",
    "        outlierConstant=3\n",
    "        a = np.array(iv)\n",
    "        upper_quartile = np.percentile(a, 75)\n",
    "        lower_quartile = np.percentile(a, 25)\n",
    "\n",
    "        IQR = (upper_quartile - lower_quartile) * outlierConstant\n",
    "        quartileSet = (lower_quartile - IQR, upper_quartile + IQR)\n",
    "        resultList = []\n",
    "        for y in a.tolist():\n",
    "#            if y >= quartileSet[0] and y <= quartileSet[1]:\n",
    "#                resultList.append(y)\n",
    "# Cap outliers\n",
    "            if y <= quartileSet[0]:\n",
    "                resultList.append(quartileSet[0])\n",
    "            elif y >= quartileSet[1]:\n",
    "                resultList.append(quartileSet[1])\n",
    "            else: \n",
    "                resultList.append(y)\n",
    "        noCap=a\n",
    "        Cap=resultList\n",
    "        return Cap\n",
    "        fig,axs = plt.subplots(1, 2)\n",
    "        for ax in axs.flat:\n",
    "            ax.set_ylim(0,8500)\n",
    "        # ax.set_xlim(0.3,1)\n",
    "        bins=20\n",
    "#        fig.suptitle(f\"{iv.name} qtiles {round(upper_quartile,1)} to {round(lower_quartile,1)} \")\n",
    "#        fig.suptitle(f\"qtiles {round(upper_quartile,1)} to {round(lower_quartile,1)} \")\n",
    "        fig.suptitle(f\"{iv.name} IQR Cap: ({round(upper_quartile,1)}) - ({round(lower_quartile,1)}) * {outlierConstant} \") \n",
    "        axs[0].hist(noCap, bins=bins,color='orange')\n",
    "        axs[0].set_title(f'Before CAP')\n",
    "        axs[0].set_ylabel(f'Number')\n",
    "        axs[0].set_xlabel(f'{iv.name}')\n",
    "    \n",
    "             \n",
    "        axs[1].hist(Cap, bins=bins,color='green')\n",
    "        axs[1].set_title(f'After CAP')\n",
    "        axs[1].set_xlabel(f'{iv.name}')\n",
    "        return resultList\n",
    "    \n",
    "    def pca(self):\n",
    "        df=pd.read_csv(argv.TK,comment= '#')\n",
    "        #.to_numpy()\n",
    "        dfnew = df[['kepid','koi_disposition','kepoi_name']].copy()     \n",
    "        X=df.drop(['kepid','koi_disposition','kepoi_name'], axis=1)\n",
    "        pca=PCA(n_components=0.95)\n",
    "        dfnew['pca']=pca.fit_transform(X)\n",
    "\n",
    "        ofile=self.TK.replace(\".csv\", \"_pca.csv\")       \n",
    "        dfnew.to_csv(ofile,index=False)\n",
    "        print(f\"{self.TK} to {ofile} with col 'pca' var ={pca.explained_variance_ratio_}\")\n",
    "        \n",
    "#    def pca_TCE1(self):\n",
    "#        df=pd.read_csv(argv.TCE1,comment= '#')\n",
    "#        #.to_numpy()\n",
    "#        dfnew = df[['kepid']].copy()     \n",
    "#        X=df.drop(['kepid'], axis=1)\n",
    "#        pca=PCA(n_components=0.95)\n",
    "#        dfnew['pca']=pca.fit_transform(X)\n",
    "\n",
    "#        ofile=self.TK.replace(\".csv\", \"_pca.csv\")       \n",
    "#        dfnew.to_csv(ofile,index=False)\n",
    "#        print(f\"{self.TK} to {ofile} with col 'pca' var ={pca.explained_variance_ratio_}\")\n",
    "\n",
    "#    def pca(self):\n",
    "#        self.pca_TK()\n",
    "#        self.pca_TCE1()\n",
    "        \n",
    "    def vif(self):\n",
    "        \"\"\" A list of highly correlate IV's (VIF>10) removed iteratively\n",
    "        usign get_vif. \"\"\"\n",
    "\n",
    "        cols=['tce_period','tce_eqt_sn','tcet_time0bk','boot_messtd','tcet_time0',\\\n",
    "                   'tce_ldm_coeff3','tce_dof1','tce_time0bk_sn','tce_max_mult_ev',\\\n",
    "                   'tce_time0','tce_smet_prov','tce_ldm_coeff2',\\\n",
    "                   'tce_rb_tcount0','tce_maxmes','tce_sma','tce_fwm_sra_sn',\\\n",
    "                   'tcet_duration','tce_chisqgofdof','tce_robstat','wst_robstat',\\\n",
    "                   'tce_period_sn','tce_smet','tce_depth','tce_dicco_mdec_sn']\n",
    "        \n",
    "        df=pd.read_csv(argv.TK,comment= '#') \n",
    "        df.drop(cols, axis=1,inplace=True)\n",
    "        fname=argv.TK.replace(\".csv\",\"_vif.csv\")   \n",
    "        df.to_csv(fname)\n",
    "        print(f\"Created {fname}\")\n",
    "        \n",
    "#        df=pd.read_csv(argv.TCE1,comment= '#') \n",
    "#        df.drop(cols, axis=1,inplace=True)\n",
    "#        fname=argv.TCE1.replace(\".csv\",\"_vif.csv\")   \n",
    "#        df.to_csv(fname)\n",
    "#        print(f\"Created {fname}\")\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    \"\"\" The _main__ models is used for testing\"\"\"\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\" Read files data/TK.csv and data/TCE1.csv created by Treat1.py. Create new file depending on options e.g --b bTK.csv, bTCE.csv --out create TK_out.csv,TCE1_out.csv \",formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "\n",
    "    group = parser.add_mutually_exclusive_group()\n",
    "    \n",
    "    group.add_argument(\"--bin\", action=\"store_true\",\n",
    "            help=\"koi_disposition (DV): Create CONFIRMED vs REST file binary(bin) add TK=>bTK (i.e merge CANDIDATE with FALSE POSITIVE)\")\n",
    "\n",
    "    group.add_argument(\"--cap\", action=\"store_true\",\n",
    "            help=\"Cap Outliers 5qtile and 95qtile. NB TCE1 is also be modified to TCE1_cap:\")\n",
    "    #argv=parser.parse_args()\n",
    "\n",
    "    class Args:\n",
    "        cap = True\n",
    "        bin = False\n",
    "        pca = False\n",
    "        npca= True\n",
    "        vif = False\n",
    "        ifile=\"data/TK.csv\"\n",
    "        TK=\"data/TK.csv\"\n",
    "        TCE1=\"data/TCE1.csv\"\n",
    "        model=\"GB\"\n",
    "    argv=Args()\n",
    "\n",
    "    df = pd.read_csv(argv.TK,comment= '#')\n",
    "\n",
    "    if(argv.bin): # No effect on TCE1 which does not have koi_disposition\n",
    "        print(f\"Mulitclass in TK={TK}\")\n",
    "        bTK,bTCE1=mytreat.bin(TK)\n",
    "        print(f\"Binary class out bTK={bTK} bTK={bTCE1}\")\n",
    "    elif(argv.vif):\n",
    "        mytreat=Treat2(argv.TK)\n",
    "        mytreat.vif()\n",
    "    elif(argv.npca):\n",
    "        #mytreat=Treat2(argv.TK)\n",
    "        #mytreat.pca()\n",
    "        \"\"\" Normalized pca \"\"\"\n",
    "        cols=['kepid','tce_plnt_num','koi_disposition','kepoi_name']\n",
    "        df=pd.read_csv(argv.ifile,comment= '#')\n",
    "        dfsave=df[df.columns & cols].copy() # Save index cols for later\n",
    "        \n",
    "        X=df.drop(cols, axis=1,errors='ignore') # Get X values for pca\n",
    "        S = StandardScaler().fit_transform(X)\n",
    "        Xstd = pd.DataFrame.from_records(S)\n",
    "       \n",
    "        pca=PCA(n_components=0.95)\n",
    "#        dfx=X.apply(pipe)\n",
    "        dfsave['pca']=pca.fit_transform(X) #  Add pca'd cols\n",
    "\n",
    "        ofile=argv.ifile.replace(\".csv\", \"_npca.csv\")\n",
    "        dfsave.to_csv(ofile,index=False)\n",
    "        print(f\"{argv.ifile} to {ofile} with col 'pca' var ={pca.explained_variance_ratio_} {pca.singular_values_}\")\n",
    "\n",
    "        \n",
    "    elif(argv.cap): # Problem tce_plnt_num returns 1 for all \n",
    "        pass\n",
    "    else: \n",
    "        print(\"Supply one argument e.g --vif\")\n",
    "\n",
    "\n",
    "#unique = df.apply(pd.Series.nunique,dropna=False)\n",
    "#Xnew=X.apply(mytreat.cap)   \n",
    "\n",
    "#df_c = pd.concat([df.reset_index(drop=True), Xnew], axis=1)\n",
    "    \n",
    "# Function to iteratively find VIF. Removed if > 10        \n",
    "#df_a=df[['kepid', 'kepoi_name','koi_disposition']].copy()\n",
    "#df_c = pd.concat([df_a.reset_index(drop=True), X], axis=1)\n",
    "        \n",
    "#fname=argv.TK.replace(\".csv\",\"_vif.csv\")               \n",
    "#df_c.to_csv(fname,index=False)\n",
    "        \n",
    "#exogs=list(X.columns)\n",
    "#pd.options.display.float_format = '{:.2f}'.format\n",
    "#vif_smf=get_vif(exogs=exogs, data=df)\n",
    "#vifs=vif_smf.sort_values(by=['VIF'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
